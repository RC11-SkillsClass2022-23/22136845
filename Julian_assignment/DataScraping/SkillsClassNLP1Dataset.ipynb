{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43b4dbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "import json\n",
    "import csv\n",
    "import urllib.request\n",
    "\n",
    "from selenium.webdriver.firefox.options import Options"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeebe5fe",
   "metadata": {},
   "source": [
    "some useful links on scraping with selenium:\n",
    "https://selenium-python.readthedocs.io/locating-elements.html\n",
    "https://www.scrapingbee.com/blog/selenium-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91ed0c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_filename = 'tactile_books.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c06cd1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create empyt csv file with dictionary headers before running the scraping \n",
    "#(Make sure not to run this with the same filename as it will erase the old file)\n",
    "\n",
    "with open(csv_filename, 'w', newline='') as csvfile:\n",
    "    fieldnames = ['title','subtitle','date','txt','gif','pdf','openURL', 'index']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames = fieldnames)\n",
    "    writer.writeheader()\n",
    "    csvfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f91136b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium.webdriver.firefox.firefox_binary import FirefoxBinary\n",
    "from selenium import webdriver\n",
    "\n",
    "options = Options()\n",
    "options.headless = False\n",
    "\n",
    "firefox_binary_path = 'C:/Program Files/Mozilla Firefox/firefox.exe'  # Update with the correct path to your Firefox binary\n",
    "\n",
    "firefox_binary = FirefoxBinary(firefox_binary_path)\n",
    "driver = webdriver.Firefox(firefox_binary=firefox_binary, executable_path='D:\\APP\\geckodriver-v0.33.0-win32\\geckodriver', options=options)\n",
    "driver.get('https://archive.org/details/texts?query=tactile&and[]=mediatype%3A%22texts%22&and[]=languageSorter%3A%22English%22&and[]=lending___status%3A%22is_readable%22')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bf054f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print(i)\n",
    "    \n",
    "    books = driver.find_elements(By.CLASS_NAME, 'item-ttl.C.C2')\n",
    "    book_links = [b.find_element(By.TAG_NAME, 'a').get_attribute('href') for b in books][-75:]\n",
    "\n",
    "\n",
    "    for b in book_links:\n",
    "        driver.get(b)\n",
    "\n",
    "        try:\n",
    "            downloads = driver.find_element(By.PARTIAL_LINK_TEXT, 'SHOW ALL')\n",
    "            downloads.click()\n",
    "        except:\n",
    "            downloads = False\n",
    "\n",
    "        if downloads:\n",
    "            try:\n",
    "                txt = driver.find_element(By.PARTIAL_LINK_TEXT, '.txt').get_attribute('href')\n",
    "            except:\n",
    "                txt = None\n",
    "            try:\n",
    "                gif = driver.find_element(By.PARTIAL_LINK_TEXT, '.gif').get_attribute('href')\n",
    "            except:\n",
    "                gif = None\n",
    "            try:\n",
    "                pdf = driver.find_element(By.PARTIAL_LINK_TEXT, '.pdf').get_attribute('href')\n",
    "            except:\n",
    "                pdf = None\n",
    "\n",
    "        driver.back()\n",
    "\n",
    "        try:\n",
    "            title = driver.find_element(By.XPATH, \"/html/body/div[1]/main/div[4]/div/div/div[2]/h1/span\").text\n",
    "        except:\n",
    "            title = None\n",
    "        try:\n",
    "            subtitle = driver.find_element(By.XPATH, \"/html/body/div[1]/main/div[4]/div/div/div[2]/dl/dd/span/a\").text\n",
    "        except:\n",
    "            subtitle = None\n",
    "        try:\n",
    "            date = driver.find_element(By.XPATH, \"/html/body/div[1]/main/div[5]/div/div/div[1]/div[2]/dl[1]/dd\").text\n",
    "        except:\n",
    "            date = None\n",
    "\n",
    "        iName = b.split('/')[-1]\n",
    "        index = 'OL'+iName+'.txt'\n",
    "\n",
    "        book = {'title':title,\n",
    "                'subtitle': subtitle,\n",
    "                'date': date,\n",
    "                'txt': txt,\n",
    "                'gif': gif,\n",
    "                'pdf': pdf,\n",
    "                'openURL':b,\n",
    "                'index': index\n",
    "               }\n",
    "\n",
    "        with open(csv_filename, 'a', newline = '', encoding='utf-8') as csvfile:\n",
    "            writer = csv.DictWriter(csvfile, fieldnames = fieldnames)\n",
    "            writer.writerow(book)\n",
    "\n",
    "        driver.back()\n",
    "    \n",
    "    driver.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5685b32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import nltk.corpus\n",
    "import random\n",
    "import os\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afeb726",
   "metadata": {},
   "source": [
    "Gensim tutorial this following part is based on:\n",
    "https://radimrehurek.com/gensim/auto_examples/core/run_core_concepts.html#sphx-glr-auto-examples-core-run-core-concepts-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b84e86ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\itsfr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8cb3027",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_books = []\n",
    "csv_filename = 'tactile_books.csv'  # Replace with your filename\n",
    "\n",
    "with open(csv_filename, newline='', encoding='utf-8') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        read_books.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8cc1ac5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Miniaturization of a microcontroller for the tactile situational awareness system.',\n",
       " 'subtitle': 'Wood, Terrence L.',\n",
       " 'date': '1999-06-01',\n",
       " 'txt': 'https://archive.org/download/miniaturizationo00wood/miniaturizationo00wood_djvu.txt',\n",
       " 'gif': 'https://archive.org/download/miniaturizationo00wood/miniaturizationo00wood.gif',\n",
       " 'pdf': 'https://archive.org/download/miniaturizationo00wood/miniaturizationo00wood.pdf',\n",
       " 'openURL': 'https://archive.org/details/miniaturizationo00wood',\n",
       " 'index': 'OLminiaturizationo00wood.txt'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_books[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94e60176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "520"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(read_books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d125361",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "def save_text(book, directory):\n",
    "    text = \"\"\n",
    "    \n",
    "    # Check if 'txt' field in book is not empty\n",
    "    if book['txt']:\n",
    "        for line in urllib.request.urlopen(book['txt']):\n",
    "            text += str(line)\n",
    "    else:\n",
    "        print(f\"No 'txt' URL found for book: {book['index']}\")\n",
    "        return\n",
    "\n",
    "    # Check if directory exists, if not, create it\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    f = open(os.path.join(directory, book['index']),\"w\")\n",
    "    f.write(text)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06abc966",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isEnglish(book, word_dictionary):\n",
    "    filename = os.path.join('books', book['index'])\n",
    "    \n",
    "    f = open(filename,'r')\n",
    "    text = f.read()\n",
    "    \n",
    "    words = text.split(' ')\n",
    "    random.shuffle(words)\n",
    "    \n",
    "    english_found = False\n",
    "    for w in words[:1000]:\n",
    "        if w in word_dictionary:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9d4478f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in read_books:\n",
    "    b = save_text(b, 'books')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9b31e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_english_books = []\n",
    "word_dictionary = nltk.corpus.words.words()\n",
    "for b in read_books:\n",
    "    if isEnglish(b, word_dictionary):\n",
    "        read_english_books.append(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7df5366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "520\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "print(len(read_books))\n",
    "print(len(read_english_books))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bbf7667c",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_paragraphs = []\n",
    "paragraph_book_index = []\n",
    "\n",
    "for b in read_english_books:\n",
    "    filename = os.path.join('books',b['index'])\n",
    "    f = open(filename, \"r\")\n",
    "    text = f.read()\n",
    "    paragraphs = text.split(\"b'\\\\n'\")\n",
    "    paragraphs2 = [x.replace(\"b'\",\"\").replace('b\"', \"\").replace(\"\\\\n'\",\"\").replace('\\\\n\"',\"\") for x in paragraphs]\n",
    "    paragraphs3 = [x.replace(\"\\\\\", \"\").replace('^', \"\") for x in paragraphs2]\n",
    "    paragraphs4 = [x for x in paragraphs3 if len(x)>100]\n",
    "    \n",
    "    for p in paragraphs4:\n",
    "        combined_paragraphs.append(p)\n",
    "        paragraph_book_index.append(b['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a595462",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "492960"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(combined_paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6429da39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This thesis examines the effects of underwater submersion and prolonged underwater submersion on a diver's tactile sensitivity. The method of constant stimulus is used to determine size discrimination thresholds. The stimuli used are squares of hard acrylic plastic into which holes of varying diameters have been drilled. \""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_paragraphs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e4c84ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this',\n",
      " 'thesis',\n",
      " 'examines',\n",
      " 'effects',\n",
      " 'underwater',\n",
      " 'submersion',\n",
      " 'prolonged',\n",
      " 'underwater',\n",
      " 'submersion',\n",
      " 'tactile',\n",
      " 'sensitivity',\n",
      " 'method',\n",
      " 'constant',\n",
      " 'stimulus',\n",
      " 'used',\n",
      " 'determine',\n",
      " 'size',\n",
      " 'discrimination',\n",
      " 'thresholds',\n",
      " 'stimuli',\n",
      " 'used',\n",
      " 'are',\n",
      " 'squares',\n",
      " 'hard',\n",
      " 'acrylic',\n",
      " 'plastic',\n",
      " 'into',\n",
      " 'which',\n",
      " 'holes',\n",
      " 'varying',\n",
      " 'diameters',\n",
      " 'have',\n",
      " 'been',\n",
      " 'drilled']\n"
     ]
    }
   ],
   "source": [
    "stoplist = set('for a of the and to in'.split(' '))\n",
    "\n",
    "# In the lecture I made an error here by combining these two lines, which added words \n",
    "#in a different form in a way they shouldn't\n",
    "\n",
    "texts = [[word.replace(\".\",\"\").replace(\",\",\"\") for word in document.lower().split()] \n",
    "         for document in combined_paragraphs]\n",
    "\n",
    "texts = [[word for word in text if (word not in stoplist and len(word)>2)] \n",
    "         for text in texts]\n",
    "\n",
    "to_delete = []\n",
    "for i in range(len(texts)):\n",
    "    t = texts[i]\n",
    "    test = [w for w in t if w.isalpha()]\n",
    "    if len(test) < 20:\n",
    "        to_delete.append(i)\n",
    "    else:\n",
    "        texts[i] = test\n",
    "\n",
    "for i in sorted(to_delete, reverse = True):\n",
    "    del texts[i]\n",
    "    del combined_paragraphs[i]\n",
    "    del paragraph_book_index[i]\n",
    "    \n",
    "# Count word frequencies\n",
    "from collections import defaultdict\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "# Only keep words that appear more than once\n",
    "processed_corpus = [[token for token in text if frequency[token] > 1] for text in texts]\n",
    "pprint.pprint(processed_corpus[0])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c397abed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the processed_corpus object to a specific directory\n",
    "with open('F:/TxtDataset/processed_corpus.pkl', 'wb') as f:\n",
    "    pickle.dump(processed_corpus, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c068460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the processed_corpus object from a specific directory\n",
    "with open('F:/TxtDataset/processed_corpus.pkl', 'rb') as f:\n",
    "    loaded_corpus = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f59d50d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
